\documentclass[0-main.tex]{subfiles}
\begin{document}

\section{Prior Work: Transition State Clustering}
\label{sec:tsc}

%\subsection{Transition State Clustering Overview}
The Transition State Clustering algorithm (TSC), learns clusters of states that mark dynamical regime transitions.

\subsection{Learning Transition States}
Let $\mathcal{D}=\{d_1,...,d_k\}$ be the set of demonstrations where each $d_i$ is a trajectory of fully observed robot states and each state is a vector in $\mathbb{R}^d$.
\tsc finds a set of transition states clusters, which are states across demonstrations associated with the same transition event, reached by a fraction of at least $\rho \in [0,1]$ of the demonstrations.
%We set $\rho$ to tolerate $1-\rho$ outliers.
We assume that demonstrations are recorded in a global fixed coordinate frame and visually from a fixed point of view and they are \emph{consistent} with at least one transition state cluster.

Transitions are defined in terms of switched linear dynamical systems (SLDS).
We model each demonstration as a SLDS:
\vspace{-3pt}
\[
\mathbf{x}(t+1) = A_{i}\mathbf{x}(t) + W(t) \text{ : } A_i \in \{A_1,...,A_k\}
\]
In this model, transitions between regimes $\{A_1,...,A_k\}$ are instantaneous where each time $t$ is associated with exactly one dynamical system matrix $1,...,k$. \emph{Transition state} is defined as the last state before a dynamical regime transition in each demonstration. A Transition state is the state $\mathbf{x}(t)$ at time $t$, such that $A(t) \ne A(t+1)$.
% Therefore, there will be times $t$ at which $A(t) \ne A(t+1)$.

Suppose there was only one regime, then we obtain a linear regression problem:
\vspace{-3pt}
\[
\arg\min_A \|A X_t - X_{t+1}\|
\]
where $X_t$ and $X_{t+1}$ are matrices with $T-1$ columns of $[\mathbf{x}(1),\mathbf{x}(2),...,\mathbf{x}(T-1)]$, and $[\mathbf{x}(2),\mathbf{x}(3),...,\mathbf{x}(T)]$ respectively.
Moldovan et al. \cite{moldovan2013dirichlet} proves that fitting a Jointly Gaussian model to $n(t) = \binom{\mathbf{x}(t+1)}{\mathbf{x}(t)}$ is equivalent to Bayesian Linear Regression.
We use Dirichlet Process Gaussian Mixture Models (DP-GMM) to learn the regimes without have to set the number of regimes in advance.
Each cluster learned signifies a different regime, and co-linear states are in the same cluster.
To find transition states, we move along a trajectory from $t=1,...,t_f$, and find states at which $n(t)$ is in a different cluster than $n(t+1)$.
These points mark a transition between clusters (i.e., transition regimes).

\subsection{Learning Transition State Clusters}
A \emph{transition state cluster} is
defined as a clustering of the set of transition states across all demonstrations; partitioning these transition states into $m$ non-overlapping similar groups:
$
\mathcal{C} = \{C_1, C_2,...,C_m\}
$
We model the states at transition states as drawn from a GMM model: ${x}(t) \sim N(\mu_i, \Sigma_i)$.
Then, we can apply the DP-GMM again to cluster the state vectors at the transition states.
Each cluster defines an ellipsoidal region of the state-space space.

\noindent Each of these clusters will have constituent vectors where each $n(t)$ belongs to a demonstration $d_i$. 
Clusters whose constituent vectors come from fewer than a fraction $\rho$ demonstrations are \emph{pruned}.

\vspace{0.5em}

\noindent \textit{Given a consistent set of demonstrations, the algorithm finds a sequence of transition state clusters reached by at least a fraction $\rho$ of the demonstrations.}

\end{document}