\section{Prior Work: Transition State Clustering}

We first overview the Transition State Clustering algorithm from our prior work \cite{krishnan2015tsc}.

\subsection{Transition State Clustering Overview}
The Transition State Clustering algorithm (\sys), learns clusters of states that mark dynamical regime transitions.

\subsubsection{Learning Transition States}
Let $\mathcal{D}=\{d_i\}$ be the set of demonstrations where each $d_i$ is a trajectory of fully observed robot states and each state is a vector in $\mathbb{R}^d$.
We model each demonstration as a switched linear dynamical system.
There is a finite set of $d \times d$ matrices $\{A_1,...,A_k\}$, and an i.i.d zero-mean additive Gaussian Markovian noise process $W(t)$ which accounts for noise in the dynamical model:
\[
\mathbf{x}(t+1) = A_{i}\mathbf{x}(t) + W(t) \text{ : } A_i \in \{A_1,...,A_k\}
\]
In this model, transitions between regimes are instantaneous where each time $t$ is associated with exactly one dynamical system matrix $1,...,k$.
\emph{Transition states} are defined as the last states before a dynamical regime transition in each demonstration.
Therefore, there will be times $t$ at which $A(t) \ne A(t+1)$.
A transition state is the state $x(t)$ at time $t$.

Suppose there was only one regime, then this would be a linear regression problem:
\[
\arg\min_A \|A X_t - X_{t+1}\|
\]
where $X_t$ is a matrix where each column vector is $x(t)$, and $X_{t+1}$ is a matrix where each column vector is the corresponding $x(t+1)$.
Moldovan et al. \cite{moldovan2013dirichlet} proves that fitting a Jointly Gaussian model to $n(t) = \binom{\mathbf{x}(t+1)}{\mathbf{x}(t)}$ is equivalent to Bayesian Linear Regression.
We use Dirichlet Process Gaussian Mixture Models (DP-GMM) to learn the regimes without have to set the number of regimes in advance.
Each cluster learned signifies a different regime, and co-linear states are in the same cluster.
To find transition states, we move along a trajectory from $t=1,...,t_f$, and find states at which $n(t)$ is in a different cluster than $n(t+1)$.
These points mark a transition between clusters (i.e., transition regimes).

\subsubsection{Learning Transition State Clusters}
A \emph{transition state cluster} is
defined as a clustering of the set of transition states across all demonstrations; partitioning these transition states into $m$ non-overlapping similar groups:
$
\mathcal{C} = \{C_1, C_2,...,C_m\}
$
In our prior work, we formalized the conditions under which meaningful clusters can be learned (i.e., consistency of demonstrations \cite{krishnan2015tsc}).
If we model the states at transition states as drawn from a GMM model: ${x}(t) \sim N(\mu_i, \Sigma_i)$.
Then, we can apply the DP-GMM again to cluster the state vectors at the transition states.
Each cluster defines an ellipsoidal region of the state-space space.

Each of these clusters will have constituent vectors where each $n(t)$ belongs to a demonstration $d_i$. 
Clusters whose constituent vectors come from fewer than a fraction $\rho$ demonstrations are \emph{pruned}.
$\rho$ should be set based on the expected rarity of outliers.

\vspace{0.5em}

\noindent \textbf{Given a consistent set of demonstrations, the algorithm finds a sequence of transition state clusters reached by at least a fraction $\rho$ of the demonstrations.}
