\documentclass[0-main.tex]{subfiles}
\begin{document}

\section{Conclusion}
The success of convolutional features in learning control policies, suggests that these features may also have other properties related to the underlying dynamical system. 
This paper explores how task segmentation can be learned from visual state representations extracted from \textit{deep} convolutional neural networks (CNNs) with a new algorithm called \tsc.
Our surprising finding is that while the original Transition State model was motivated for spatial dynamical systems, it also empirically performs well (in comparison to ground truth) even when applied to trajectories of visual filters derived from layers of CNNs.
However, this required several novel contributions including hierarchical clustering and dimensionality reduction. 
On real datasets, we find that \tsc matches the manual annotation with up to 0.806 NMI, and our results also suggest that including kinematics and vision results in increases of up-to 0.215 NMI over kinematics alone.


More importantly, they also suggest a number of important directions for future work.
The CNNs applied in this work are optimized for image classification of natural images and not the images seen in surgery.
In future work, we will explore training CNNs (as opposed to pre-trained) to identify features directly from both pixels and kinematics. 
However, segmentation is a means to an end, and we hope to connect our work with improved policy learning and hierachical planning.
%in \tsc, we apply pre-trained CNNs for visual featurization.



\end{document}