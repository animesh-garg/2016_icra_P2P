\section{Latent State $H_t$}
Describe the process of linking kinematics and video (PCA, CCA, etc.).
If we apply any VLAD or encoding, or batching, describe it here:

\subsection{Temporal Batching}
We have so far used the concatenation of kinematic and visual features as the state representation $\binom{k(t)}{z(t)}$. However, the use of rolling time window in both kinematics and visual space allows capturing dynamics. Explicit use of first and second numerical derivatives in state representation for learning complex dynamics has been the state-of the art\tocite. 

We propose the use of a sequence raw states as the representation. Addition of every time step in the state implicitly corresponds to adding derivatives. However, the benefit of adding more time history saturates while the computational intensity requires scales quickly, especially with the use of high dimensional image features. 

We propose the use of $[k(t-1), k(t), k(t+1)]^T$ as the kinematic state representation. Other lengths of temporal history were experimented with as shown in Figure~\todo{add figure}. We note that while we use a 3 step temporal history, with data being captured at 30Hz, the amount of movement may be negligible between consecutive frames. We sub-sample the kinematic data to 10Hz to magnify kinematic changes. However, we keep the video data at 30Hz and use 9 consecutive frames for each $z(t)$. Using a CNN based featurization results in a very high dimensional feature vector, while use of SIFT like features results in a a feature vector of different lengths across different frames. Hence an encoding procedure as described following in employed to recover a concise and fixed dimensional representation for video subsequences. 

\todo{Current Status}: While batching is supposed to help in video analysis , I've seen good separation of clusters (PCA on conv features) with just individual frames without clustering... I think We need to look into this more and need to test it out with milestones clustering.


\subsection{Video Encoding}
Vector of Locally Aggregated Descriptors (VLAD) image encoding as proposed by \cite{arandjelovic2013all}\ignore{\cite{jegou2010aggregating}} is a feature encoding and pooling method, similar to Fisher vectors. 
It transforms an incoming variable-size set of independent samples into a fixed size vector representation.
VLAD was recently shown to perform better than Fisher vectors and average pooling for encoding multiple frames~\cite{xu2014discriminative}.

VLAD encodes a set of local feature descriptors $I=\{x_1,\ldots,x_n\}$ extracted from an image using a code book $\mathrm{C} = \{c_1, \ldots, c_K \}$ built using a clustering method such as Gaussian Mixture Models (GMM) or K-means clustering. With K-coarse cluster centers, generated by K-means, we can obtain the difference vector for every center $c_j$ as:
\[ u_k = \sum_{i:\,1\text{-}nbd(x_i)=\{c_j\}} (x_i- c_j)
\]
where $k\text{-}nbd(x_i)$ indicates k(=1) nearest neighbors of $x_i$ among K coarse centers.
VLAD vector is obtained by concatenating $u_k$ over all the K centers, $V = [u_1^T, \ldots, u_K^T]$, with $V$ of size K$D$ where $D$ is the dimension of incoming points $x_i$. 

Further, \cite{kantorov2014efficient} showed that VLAD-k, an extension to k-nearest neighbors outperforms vanilla VLAD (k=1). VLAD vectors are generally normalized using power normalization ($sign(u_k)\sqrt{\|u_k\|_2}$) followed by $L_2$ normalization of $V$. However an intra-normalization has been shown to perform better in balancing all the features~\cite{arandjelovic2013all}. This entails: $\hat{u}_k = u_k/\|u_k\|_2$ followed by $L_2$ normalization of $V$.
We use VLAD-k with k=5 followed by intra-normalization.

\subsection*{Latent Concept Descriptors}

It is worth noting that convolutional filters can be regarded as generalized linear classifiers on spatial patches, and each conv filter can be equated to a latent concept as proposed in~\cite{xu2014discriminative}. We represent the features from convolutional layers as vectors of latent concepts, with each entry representing a response to a latent concept. Since, conv filters at every layer are independent, each filter response can be understood as the prediction on linear classifier on respective latent concept.

Convolutional layers contain spatial information, however at the cost of high dimensional representation. For instance the feature dimension at a convolutional layer can be $c\times c \times M$, where $c$ is filter size and $M$ is number of conv filters. Simply flattening the layer would result in a high dimensional representation inducing computational instability. Since convolutional layers are higher dimensional than fully connected layers, they are often not used directly.

\todo{re-phrase}
However using the latent concept descriptors, a conv layer of size $c\times c \times M$ can be converted into $c^2$ latent concept descriptors with M dimensions. Each latent concept descriptor repre- sents the responses from the M filters for a specific pool- ing location. Once we obtain the latent concept descriptors for all the frames in a video, we then apply an encoding method to generate the video representation. In this case, each frame contains a2 descriptors instead of one descriptor for the frame.

\todo{Current Status}: We have also implemented the following encoding method- Latent Content Descriptors (LCD)+ VLAD \cite{xu2014discriminative}. However, initial results weren't super but we need to see how it performs on milestone clustering.

\subsection{Encoding Dimensionality}
The output of the encoding step is of the dimension $DK$, with D being the dimension of incoming points. Particularly in case of CNN features drawn from convolutional layers can run into tens of thousands leading to imbalance in the vectors after concatenation with kinematics. 

Hence 


